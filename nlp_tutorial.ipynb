{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOboLys5QBkwO8Ct52/yqwT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syalal/6.00.1x/blob/master/nlp_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wogMFZYRLvgb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "texts = [\"Hi, how are you #hi\", \"BERT is a new package developed by Google\", \"NLP is Natural Language Processing #nlp\", \"This is my colab notebook #colab #notebook\"]\n",
        "df1 = pd.DataFrame(texts, columns = ['text'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLk8avSMMBv2",
        "colab_type": "code",
        "outputId": "572c1ec9-8683-4a77-c59e-4786ea3e8115",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "df1.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hi, how are you #hi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BERT is a new package developed by Google</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NLP is Natural Language Processing #nlp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>This is my colab notebook #colab #notebook</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         text\n",
              "0                         Hi, how are you #hi\n",
              "1   BERT is a new package developed by Google\n",
              "2     NLP is Natural Language Processing #nlp\n",
              "3  This is my colab notebook #colab #notebook"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgukFa8DMo2p",
        "colab_type": "text"
      },
      "source": [
        "###**Count number of words with hashtags in each sentence in dataframe**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBifG-oYMm1a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def hashtag_count(sentence):\n",
        "  hashtags = [word for word in sentence if word.startswith('#')]\n",
        "\n",
        "  return len(hashtags)no\n",
        "\n",
        "df1['Count of Hashtags'] = df1['text'].apply(hashtag_count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAQJDf2TNYCa",
        "colab_type": "code",
        "outputId": "e02cbe03-9a3a-4a73-9be2-f91443431571",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "df1.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>Count of Hashtags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hi, how are you #hi</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BERT is a new package developed by Google</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NLP is Natural Language Processing #nlp</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>This is my colab notebook #colab #notebook</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         text  Count of Hashtags\n",
              "0                         Hi, how are you #hi                  1\n",
              "1   BERT is a new package developed by Google                  0\n",
              "2     NLP is Natural Language Processing #nlp                  1\n",
              "3  This is my colab notebook #colab #notebook                  2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8NnseSmNezH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUJNdbMsWxjs",
        "colab_type": "text"
      },
      "source": [
        "Text Preprocessing steps:\n",
        "\n",
        "1. converting all letters to lower or upper case\n",
        "2. converting numbers into words or removing numbers\n",
        "3. removing punctuations, accent marks and other diacritics\n",
        "4. removing white spaces\n",
        "5. expanding abbreviations\n",
        "6. removing stop words, sparse terms, and particular words\n",
        "7. text canonicalization\n",
        "\n",
        "Source: https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2SFQF_l82TL",
        "colab_type": "text"
      },
      "source": [
        "Example 1. Convert text to lowercase\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxVOvUm07AT6",
        "colab_type": "code",
        "outputId": "5ddb87e4-f1dc-474b-a22f-e3ef343f96a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "input_str = \"The 5 biggest countries by population in 2017 are China, India, United States, Indonesia, and Brazil.\"\n",
        "input_str = input_str.lower()\n",
        "print(input_str)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the 5 biggest countries by population in 2017 are china, india, united states, indonesia, and brazil.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ro0ZZi8onO4B",
        "colab_type": "text"
      },
      "source": [
        "**1.1 Convert Accented Characters**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfgmOqv9nWk_",
        "colab_type": "code",
        "outputId": "37ce24f8-c0af-4239-8dcf-7e1ce9672852",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "import unidecode\n",
        "from word2number import w2n\n",
        "# from pycontractions import Contractions\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "    \"\"\"remove accented characters from text, e.g. café\"\"\"\n",
        "    text = unidecode.unidecode(text)\n",
        "    return text\n",
        "\n",
        "text = \"Would you like to have latté at our café?\"\n",
        "\n",
        "remove_accented_chars(text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Would you like to have latte at our cafe?'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNMvhyCsskmd",
        "colab_type": "text"
      },
      "source": [
        "#### **1.2 Expand Contractions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZZOHBe7skPu",
        "colab_type": "code",
        "outputId": "d28ace23-fd41-43f5-cd86-4be4a12ca0f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "from pycontractions import Contractions\n",
        "\n",
        "cont = Contractions(kv_model=model)\n",
        "cont.load_models()\n",
        "\n",
        "def expand_contractions(text):\n",
        "    \"\"\"expand shortened words, e.g. don't to do not\"\"\"\n",
        "    text = list(cont.expand_texts([text], precise=True))[0]\n",
        "    return text\n",
        "\n",
        "expand_contractions(text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-5808b6b36b93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpycontractions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mContractions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcont\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mContractions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkv_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcont\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pycontractions'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bu178p6UvOXv",
        "colab_type": "text"
      },
      "source": [
        "**1.3 word to num**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx_TxjjctbHI",
        "colab_type": "code",
        "outputId": "278f1bb2-47ad-45a3-aa77-9fc982b2a35c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "import spacy\n",
        "\n",
        "text = \"\"\"three cups of coffee\"\"\"\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "\n",
        "doc = nlp(text)\n",
        "tokens = [w2n.word_to_num(token.text) if token.pos_ == 'NUM' else token for token in doc]\n",
        "tokens"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-6e9dc4b9966a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\"three cups of coffee\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_md'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_md'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGWKSLRL8-XT",
        "colab_type": "text"
      },
      "source": [
        "Example 2. Numbers removing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDnegtqu87kT",
        "colab_type": "code",
        "outputId": "1d6dbad1-6486-4413-c08c-f87dd658b470",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import re\n",
        "input_str = \"Box A contains 3 red and 5 white balls, while Box B contains 4 red and 2 blue balls.\"\n",
        "result = re.sub(r'\\d+', '', input_str)\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Box A contains  red and  white balls, while Box B contains  red and  blue balls.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ufqowuc97W7",
        "colab_type": "text"
      },
      "source": [
        "Example 3. Punctuation removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UbBFcAo9xP4",
        "colab_type": "code",
        "outputId": "71649457-f63b-4016-9281-7797412a82bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import string\n",
        "input_str = \"This &is [an] example? {of} string. with.? punctuation!!!!\"\n",
        "\n",
        "result = re.sub(r'[^\\w\\s]', '', input_str)\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is an example of string with punctuation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OO6q3K98GRxC",
        "colab_type": "text"
      },
      "source": [
        "Example 4. White spaces removal\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd1Icnnk_EG8",
        "colab_type": "code",
        "outputId": "30d29116-c0e6-4310-e832-35368252fb39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "input_str = \"\\t a string example\\t\"\n",
        "input_str = input_str.strip()\n",
        "input_str"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a string example'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3rQlbpzHSXi",
        "colab_type": "text"
      },
      "source": [
        "Example 7. Stop words removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQdnncg1GU9Z",
        "colab_type": "code",
        "outputId": "caf9fc42-91d3-4ce5-b72d-7c70728c6f6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "input_str = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "print(stop_words)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'do', 'other', 'and', 'been', 'he', 'd', \"you're\", 'an', 'until', 'but', 'not', 'by', \"you'd\", 'm', 'himself', 'own', 'at', 'very', 'was', 'with', 'so', 'mightn', 'needn', 'didn', 'themselves', 'few', \"should've\", 'wouldn', 'were', 'y', \"aren't\", \"it's\", 'yours', 'won', 'as', 'there', 'this', 'has', 'can', 've', \"wouldn't\", \"you'll\", 'mustn', 'weren', \"couldn't\", 'now', \"she's\", 'through', 'them', 'who', 'between', \"you've\", 'too', 'doesn', 'further', 'about', 'will', 'those', 'over', 'me', 'ourselves', 'no', \"isn't\", 'your', 'ma', 'have', 'nor', 'or', 'aren', 'that', 'during', 'here', 're', 'for', 'his', 'couldn', 'am', 'where', \"shouldn't\", 'it', 'doing', 'don', 'than', 'hasn', 'shouldn', 'herself', 'which', 'having', 'ain', 'such', \"mustn't\", 'any', 'i', 'her', \"don't\", 'under', 'again', 'down', 's', 'just', 'isn', 'of', 'she', \"hadn't\", 'below', \"weren't\", 'above', 'its', 'a', 'in', 'yourselves', 'does', \"mightn't\", 'whom', 'wasn', 'on', 'how', \"shan't\", 'theirs', 'being', 'some', 'only', 'my', 'more', 'up', 'are', 'same', \"needn't\", 'their', 'is', 'when', 'why', \"won't\", \"haven't\", 'after', 'shan', 'once', 'be', \"doesn't\", 'you', 'to', 'off', 'haven', \"wasn't\", 'ours', 'out', 'against', 'had', 'hadn', 'all', \"that'll\", 'then', 'while', 'if', 'each', 'because', 'itself', 'did', 'hers', 'our', 'from', 'we', 'they', 't', 'most', 'these', 'both', \"didn't\", 'myself', 'yourself', 'into', 'o', 'him', \"hasn't\", 'll', 'should', 'what', 'the', 'before'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8iD8zYqIGRA",
        "colab_type": "code",
        "outputId": "5e63d1fc-ba2b-456b-8623-9ac6f2773f76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokens = word_tokenize(input_str)\n",
        "print(tokens)\n",
        "result = [i for i in tokens if not i in stop_words]\n",
        "print (result)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.']\n",
            "['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32BV-UuSIfJ6",
        "colab_type": "code",
        "outputId": "2044fc1f-e32c-41b9-8785-d8eeaf073cd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## remove extra spacings\n",
        "\n",
        "s=\"\\n      My name is John\\n      and I like to go.\\n      blahblahblah.\\n         \\n\\n      \"\n",
        "\n",
        "r = re.sub(r'\\s+', r\" \", s)\n",
        "print(r)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " My name is John and I like to go. blahblahblah. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgM1lSk3Lo_e",
        "colab_type": "text"
      },
      "source": [
        "Example 8. Stemming using NLTK:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzuAyw0KKY8g",
        "colab_type": "code",
        "outputId": "0b2f2792-88e4-41a8-f706-38f2007fb441",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "input_str = \"There are several types of stemming algorithms.\"\n",
        "\n",
        "tkn = word_tokenize(input_str)\n",
        "\n",
        "output = [stemmer.stem(word) for word in tkn]\n",
        "print(output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['there', 'are', 'sever', 'type', 'of', 'stem', 'algorithm', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfQ636SENx53",
        "colab_type": "text"
      },
      "source": [
        "Example 9. Lemmatization using NLTK:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AummzkhYMNRs",
        "colab_type": "code",
        "outputId": "edd3243a-5909-4f7d-b74b-292e4f5423ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "input_str = \"been had done languages cities mice\"\n",
        "\n",
        "tkn = word_tokenize(input_str)\n",
        "r = [lemmatizer.lemmatize(word) for word in tkn]\n",
        "print(r)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "['been', 'had', 'done', 'language', 'city', 'mouse']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bE38a82Oq6W",
        "colab_type": "text"
      },
      "source": [
        "Example 10. Part-of-speech tagging using TextBlob:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1TjaHAbOVDJ",
        "colab_type": "code",
        "outputId": "6cabd224-3fd8-4ec9-f5f6-300c1f19e76f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# using textblob\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "input_str=\"Parts of speech examples: an article, to write, interesting, easily, and, of\"\n",
        "\n",
        "from textblob import TextBlob\n",
        "result = TextBlob(input_str)\n",
        "print(result.tags)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Parts', 'NNS'), ('of', 'IN'), ('speech', 'NN'), ('examples', 'NNS'), ('an', 'DT'), ('article', 'NN'), ('to', 'TO'), ('write', 'VB'), ('interesting', 'VBG'), ('easily', 'RB'), ('and', 'CC'), ('of', 'IN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXNjHk23RII1",
        "colab_type": "text"
      },
      "source": [
        "Example 11. Chunking using NLTK:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VeIN09dP9A6",
        "colab_type": "code",
        "outputId": "998ee696-107e-4c56-8766-a5c0ad491d8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "# first step is POS tagging\n",
        "\n",
        "input_str=\"A black television and a white stove were bought for the new apartment of John.\"\n",
        "from textblob import TextBlob\n",
        "result = TextBlob(input_str)\n",
        "print(result.tags)\n",
        "\n",
        "reg_exp = “NP: {<DT>?<JJ>*<NN>}”\n",
        "rp = nltk.RegexpParser(reg_exp)\n",
        "result = rp.parse(result.tags)\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-47-8fee2aa6fd31>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    reg_exp = “NP: {<DT>?<JJ>*<NN>}”\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwgpZwmARRTo",
        "colab_type": "code",
        "outputId": "85730ce9-9b59-462c-f8b9-8310f826353a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "source": [
        "result.draw()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-6828c246cf50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'TextBlob' object has no attribute 'draw'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4IXPSCPR2Eu",
        "colab_type": "text"
      },
      "source": [
        "**Example** 12. **Named-entity recognition(NER)** using NLTK:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3A9-cdSRsAN",
        "colab_type": "code",
        "outputId": "4f074449-4eea-4b65-fb55-26eefbf857be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# nltk.download('maxent_ne_chunker')\n",
        "# nltk.download('words')\n",
        "# import nltk\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "\n",
        "input_str = \"Bill works for Apple so he went to Boston for a conference.\"\n",
        "print((pos_tag(word_tokenize(input_str))))\n",
        "#ne_chunk\n",
        "print(ne_chunk(pos_tag(word_tokenize(input_str))))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Bill', 'NNP'), ('works', 'VBZ'), ('for', 'IN'), ('Apple', 'NNP'), ('so', 'IN'), ('he', 'PRP'), ('went', 'VBD'), ('to', 'TO'), ('Boston', 'NNP'), ('for', 'IN'), ('a', 'DT'), ('conference', 'NN'), ('.', '.')]\n",
            "(S\n",
            "  (PERSON Bill/NNP)\n",
            "  works/VBZ\n",
            "  for/IN\n",
            "  Apple/NNP\n",
            "  so/IN\n",
            "  he/PRP\n",
            "  went/VBD\n",
            "  to/TO\n",
            "  (GPE Boston/NNP)\n",
            "  for/IN\n",
            "  a/DT\n",
            "  conference/NN\n",
            "  ./.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHAeAnWGMYTA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1a865d07-d8cd-4ebb-eac5-216082259a58"
      },
      "source": [
        "# NER with visualization\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "sentence = \"Bill works for Apple so he went to Boston for a conference.\"\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "sentence_nlp = nlp(sentence)\n",
        "\n",
        "# print named entities in article\n",
        "print([(word, word.ent_type_) for word in sentence_nlp if word.ent_type_])\n",
        "\n",
        "# visualize named entities\n",
        "displacy.render(sentence_nlp, style='ent', jupyter=True)\n",
        "\n",
        "# https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(Bill, 'PERSON'), (Apple, 'ORG'), (Boston, 'GPE')]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Bill\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " works for \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Apple\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " so he went to \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Boston\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " for a conference.</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3RXRXdtSbwM",
        "colab_type": "code",
        "outputId": "fc50bea3-0a47-4262-d467-5af6a1c24ba2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#lemmatization using scapy\n",
        "\n",
        "blog = '\\nTwenty-first-century politics has witnessed an alarming rise of populism in the U.S. and Europe. The first warning signs came with the UK Brexit Referendum vote in 2016 swinging in the way of Leave. This was followed by a stupendous victory by billionaire Donald Trump to become the 45th President of the United States in November 2016. Since then, Europe has seen a steady rise in populist and far-right parties that have capitalized on Europe’s Immigration Crisis to raise nationalist and anti-Europe sentiments. Some instances include Alternative for Germany (AfD) winning 12.6% of all seats and entering the Bundestag, thus upsetting Germany’s political order for the first time since the Second World War, the success of the Five Star Movement in Italy and the surge in popularity of neo-nazism and neo-fascism in countries such as Hungary, Czech Republic, Poland and Austria.\\n'\n",
        "\n",
        "import spacy\n",
        "\n",
        "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "# Load model and create Doc object\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(blog)\n",
        "\n",
        "# Generate lemmatized tokens\n",
        "lemmas = [token.lemma_ for token in doc]\n",
        "\n",
        "# Remove stopwords and non-alphabetic tokens\n",
        "a_lemmas = [lemma for lemma in lemmas \n",
        "            if lemma.isalpha() and lemma not in stopwords]\n",
        "\n",
        "# Print string after text cleaning\n",
        "print(' '.join(a_lemmas))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "century politic witness alarming rise populism Europe warning sign come UK Brexit Referendum vote swinging way Leave follow stupendous victory billionaire Donald Trump President United States November Europe steady rise populist far right party capitalize Europe Immigration Crisis raise nationalist anti europe sentiment instance include Alternative Germany AfD win seat enter Bundestag upsetting Germany political order time Second World War success Five Star Movement Italy surge popularity neo nazism neo fascism country Hungary Czech Republic Poland Austria\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0YtQae2ycNt",
        "colab_type": "code",
        "outputId": "4f766fe4-4b5b-4c50-bfae-5880980b1f4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "# Function to preprocess text in dataframe\n",
        "ted = pd.DataFrame()\n",
        "\n",
        "def preprocess(text):\n",
        "  \t# Create Doc object\n",
        "    doc = nlp(text, disable=['ner', 'parser'])\n",
        "    # Generate lemmas\n",
        "    lemmas = [token.lemma_ for token in doc]\n",
        "    # Remove stopwords and non-alphabetic characters\n",
        "    a_lemmas = [lemma for lemma in lemmas \n",
        "            if lemma.isalpha() and lemma not in stopwords]\n",
        "    \n",
        "    return ' '.join(a_lemmas)\n",
        "  \n",
        "# Apply preprocess to ted['transcript']\n",
        "ted['transcript'] = ted['transcript'].apply(preprocess)\n",
        "print(ted['transcript'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-24e63313a73e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Apply preprocess to ted['transcript']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'transcript'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'transcript'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'transcript'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ted' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o64sSiYD9c2F",
        "colab_type": "text"
      },
      "source": [
        "**Part of Speech(POS) Tagging** using **spacy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrtPqQdU2PB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Part of Speech(POS) Tagging using spacy\n",
        "\n",
        "txt = \"Ram is an expert in NLP\"\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "doc = nlp(txt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGRLTkFU3Rvv",
        "colab_type": "code",
        "outputId": "66e58660-c409-408c-a3d6-6fb4cd5300c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pos = [(token.text, token.pos_) for token in doc]\n",
        "print(pos)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Ram', 'PROPN'), ('is', 'VERB'), ('an', 'DET'), ('expert', 'NOUN'), ('in', 'ADP'), ('NLP', 'PROPN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjPKZwZM9rdN",
        "colab_type": "text"
      },
      "source": [
        "**Named Entity Recognition(NER) using spacy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjQl3J393cQn",
        "colab_type": "code",
        "outputId": "527aac76-950e-428c-ee63-62f773b9e861",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "txt = \"Jim is a Data Analyst at DBS. He lives in Singapore\"\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(txt)\n",
        "\n",
        "ne = [(token.text, token.label_) for token in doc.ents]\n",
        "print(ne)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Jim', 'PERSON'), ('DBS', 'ORG'), ('Singapore', 'GPE')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKMzACmrEvIc",
        "colab_type": "text"
      },
      "source": [
        "**Word Vectorization** Bag of words using sklearn\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdNzTSGm-uQA",
        "colab_type": "code",
        "outputId": "5c31dfe9-f8a5-4bd9-d86e-d9bae8911669",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = pd.Series(['Lion is King of jungle', 'Lions have lifespan of a decade', 'The lion is an endangered species'])\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(bow_matrix.toarray())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 0 1 1 1 0 1 0 1 0 0]\n",
            " [0 1 0 1 0 0 0 1 0 1 1 0 0]\n",
            " [1 0 1 0 1 0 0 0 1 0 0 1 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ucCT6tH-vTo",
        "colab_type": "code",
        "outputId": "5afdab65-965a-40b5-bc60-c61563834ce2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "# Convert bow_matrix into a DataFrame\n",
        "bow_df = pd.DataFrame(bow_matrix.toarray())\n",
        "\n",
        "# Map the column names to vocabulary \n",
        "bow_df.columns = vectorizer.get_feature_names()\n",
        "\n",
        "print(bow_df.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   an  decade  endangered  have  is  ...  lion  lions  of  species  the\n",
            "0   0       0           0     0   1  ...     1      0   1        0    0\n",
            "1   0       1           0     1   0  ...     0      1   1        0    0\n",
            "2   1       0           1     0   1  ...     1      0   0        1    1\n",
            "\n",
            "[3 rows x 13 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIM65_tqOZFU",
        "colab_type": "text"
      },
      "source": [
        "**Building a BoW Naive Bayes classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO8PdXqTIxPg",
        "colab_type": "code",
        "outputId": "13dc79c1-cf7d-44be-a0f9-4b7c94ba41eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorize = CountVectorizer(strip_accents = 'ascii', lowercase = False, stop_words = 'english')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(df['message'], df['label'], test_size = 0.25)\n",
        "\n",
        "x_train_bow = vectorize.fit_transform(x_train)\n",
        "x_test_bow = vectorize.tranform(x_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-5e6125e66a4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mx_train_bow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkzusKRSQTDU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training Naive Bayes Classifier\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "clf = MultinomialNB()\n",
        "\n",
        "#train clf\n",
        "clf.fit(x_train_bow, y_train)\n",
        "\n",
        "#compute accuracy on test set\n",
        "accuracy = clf.score(x_test_bow, y_test)\n",
        "print(accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2u3xiQLuuuA5",
        "colab_type": "text"
      },
      "source": [
        "**n-gram models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3a1r_RWRGtv",
        "colab_type": "code",
        "outputId": "537b7382-b71c-474c-a8d7-1004e5805d3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "corpus = pd.Series(['Lion is King of jungle', 'Lions have lifespan of a decade', 'The lion is an endangered species'])\n",
        "\n",
        "# Generate n-grams upto n=1\n",
        "vectorizer_ng1 = CountVectorizer(ngram_range=(1,1))\n",
        "ng1 = vectorizer_ng1.fit_transform(corpus)\n",
        "\n",
        "# Generate n-grams upto n=2\n",
        "vectorizer_ng2 = CountVectorizer(ngram_range=(1,2))\n",
        "ng2 = vectorizer_ng2.fit_transform(corpus)\n",
        "\n",
        "# Generate n-grams upto n=3\n",
        "vectorizer_ng3 = CountVectorizer(ngram_range=(1,3))\n",
        "ng3 = vectorizer_ng3.fit_transform(corpus)\n",
        "\n",
        "# Print the number of features for each model\n",
        "print(\"ng1, ng2 and ng3 have %i, %i and %i features respectively\" % (ng1.shape[1], ng2.shape[1], ng3.shape[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ng1, ng2 and ng3 have 13, 25 and 35 features respectively\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b6e3to9Ob75",
        "colab_type": "code",
        "outputId": "6a9be67a-08ff-4249-8f0a-4be9d20bf207",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "ng1.toarray()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0],\n",
              "       [0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0],\n",
              "       [1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48l-l7NP2ehY",
        "colab_type": "text"
      },
      "source": [
        "Higher order n-grams for sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S23MVeM1wM6v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(df['message'], df['label'], test_size = 0.25)\n",
        "\n",
        "X_train_ng = vectorize.fit_transform(x_train)\n",
        "X_test_ng = vectorize.tranform(x_test)\n",
        "\n",
        "# Define an instance of MultinomialNB \n",
        "clf_ng = MultinomialNB()\n",
        "\n",
        "# Fit the classifier \n",
        "clf_ng.fit(X_train_ng, y_train)\n",
        "\n",
        "# Measure the accuracy \n",
        "accuracy = clf_ng.score(X_test_ng, y_test)\n",
        "print(\"The accuracy of the classifier on the test set is %.3f\" % accuracy)\n",
        "\n",
        "# Predict the sentiment of a negative review\n",
        "review = \"The movie was not good. The plot had several holes and the acting lacked panache.\"\n",
        "prediction = clf_ng.predict(ng_vectorizer.transform([review]))[0]\n",
        "print(\"The sentiment predicted by the classifier is %i\" % (prediction))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liQYDYuS6D3P",
        "colab_type": "text"
      },
      "source": [
        "**Term frequency - Inverse document frequency (tf - Idf)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7nNrNL52fjT",
        "colab_type": "code",
        "outputId": "e4655ab1-b680-48f3-84fb-dd2123eb13cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "print(tfidf_matrix.toarray())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.         0.         0.         0.3935112  0.51741994\n",
            "  0.51741994 0.         0.3935112  0.         0.3935112  0.\n",
            "  0.        ]\n",
            " [0.         0.46735098 0.         0.46735098 0.         0.\n",
            "  0.         0.46735098 0.         0.46735098 0.35543247 0.\n",
            "  0.        ]\n",
            " [0.44036207 0.         0.44036207 0.         0.3349067  0.\n",
            "  0.         0.         0.3349067  0.         0.         0.44036207\n",
            "  0.44036207]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuKTFMUUJzeM",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f6RWtZx_AhR",
        "colab_type": "text"
      },
      "source": [
        "**Cosine Similarity**\n",
        "\n",
        "similarity between two vectors. value between 0 to 1. 0 means not matching. 1 means matching\n",
        "\n",
        "Cos 0 = A.B/|A||B|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNeb37hV7f4S",
        "colab_type": "code",
        "outputId": "f7a857ac-16c3-40e2-bc29-b885688bd384",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "a = (4, 5, 6)\n",
        "b = (2, 3, 4)\n",
        "\n",
        "score = cosine_similarity([a], [b])\n",
        "print(score)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.99461155]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO66ZuhkJ0Xl",
        "colab_type": "text"
      },
      "source": [
        "**Building a plot line based recommender**\n",
        "\n",
        "steps:\n",
        "\n",
        "> Text preprocessing\n",
        "\n",
        "> Generate tf-idf vectors\n",
        "\n",
        "> Generate pair-wise Cosine Similarity marix of the tf-idf matrix -- (cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "> Build recommender function\n",
        "\n",
        " both linear_kernel and cosine_similarity produced the same result.\n",
        " When you're working with a very large amount of data and your vectors are in the tf-idf representation, it is good practice to default to **linear_kernel** to improve performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePHSxfCUEA2c",
        "colab_type": "code",
        "outputId": "3b519d89-dcad-4b4b-891c-b74021ef6612",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "indices = pd.Series(['The Dark Knight Rises', 'Batman Forever', 'Batman', 'Jumanji', 'Waiting to Exhale'])\n",
        "\n",
        "# Initialize the TfidfVectorizer \n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "movie_plots = pd.Series([\"Following the death of District Attorney Harvey Dent, Batman assumes responsibility for Dents crimes to protect the late attorney's reputation\", \n",
        "                         \"batman ijd,h\", 'dmgj bdk batman', 'forest jumanji game', 'wait serious'])\n",
        "\n",
        "# Construct the TF-IDF matrix\n",
        "tfidf_matrix = tfidf.fit_transform(movie_plots)\n",
        "\n",
        "# Generate the cosine similarity matrix\n",
        "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        " \n",
        "# Generate recommendations \n",
        "print(get_recommendations('The Dark Knight Rises', cosine_sim, indices))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-f9e3c5e93373>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Generate recommendations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_recommendations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The Dark Knight Rises'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosine_sim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'get_recommendations' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLaeQSf7nChf",
        "colab_type": "text"
      },
      "source": [
        "*POS Tagging using spacy and nltk*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkDpToolZKW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a basic pre-processed corpus, don't lowercase to get POS context\n",
        "corpus = normalize_corpus(news_df['full_text'], text_lower_case=False, \n",
        "                          text_lemmatization=False, special_char_removal=False)\n",
        "\n",
        "# demo for POS tagging for sample news headline\n",
        "sentence = str(news_df.iloc[1].news_headline)\n",
        "sentence_nlp = nlp(sentence)\n",
        "\n",
        "# POS tagging with Spacy \n",
        "spacy_pos_tagged = [(word, word.tag_, word.pos_) for word in sentence_nlp]\n",
        "pd.DataFrame(spacy_pos_tagged, columns=['Word', 'POS tag', 'Tag type'])\n",
        "\n",
        "# POS tagging with nltk\n",
        "nltk_pos_tagged = nltk.pos_tag(sentence.split())\n",
        "pd.DataFrame(nltk_pos_tagged, columns=['Word', 'POS tag'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSNGYI23FV1z",
        "colab_type": "text"
      },
      "source": [
        "**Dependency Parsing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9plC8QPFWyI",
        "colab_type": "code",
        "outputId": "f948fd39-b080-44db-9722-9adcb36d404d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 826
        }
      },
      "source": [
        "import stanfordnlp\n",
        "stanfordnlp.download('en')   # This downloads the English models for the neural pipeline\n",
        "nlp = stanfordnlp.Pipeline() # This sets up a default neural pipeline in English\n",
        "doc = nlp(\"Barack Obama was born in Hawaii.  He was elected president in 2008.\")\n",
        "doc.sentences[0].print_dependencies()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using the default treebank \"en_ewt\" for language \"en\".\n",
            "Would you like to download the models for: en_ewt now? (Y/n)\n",
            "Y\n",
            "\n",
            "Default download directory: /root/stanfordnlp_resources\n",
            "Hit enter to continue or type an alternate directory.\n",
            "\n",
            "\n",
            "Downloading models for: en_ewt\n",
            "Download location: /root/stanfordnlp_resources/en_ewt_models.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235M/235M [00:32<00:00, 6.65MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Download complete.  Models saved to: /root/stanfordnlp_resources/en_ewt_models.zip\n",
            "Extracting models file for: en_ewt\n",
            "Cleaning up...Done.\n",
            "Use device: cpu\n",
            "---\n",
            "Loading: tokenize\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "---\n",
            "Loading: pos\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "---\n",
            "Loading: lemma\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "Building an attentional Seq2Seq model...\n",
            "Using a Bi-LSTM encoder\n",
            "Using soft attention for LSTM.\n",
            "Finetune all embeddings.\n",
            "[Running seq2seq lemmatizer with edit classifier]\n",
            "---\n",
            "Loading: depparse\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_parser.pt', 'pretrain_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "Done loading processors!\n",
            "---\n",
            "('Barack', '4', 'nsubj:pass')\n",
            "('Obama', '1', 'flat')\n",
            "('was', '4', 'aux:pass')\n",
            "('born', '0', 'root')\n",
            "('in', '6', 'case')\n",
            "('Hawaii', '4', 'obl')\n",
            "('.', '4', 'punct')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EU0G-d3wFZgZ",
        "colab_type": "code",
        "outputId": "354714ee-c279-411f-f71b-26620a75d333",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "!pip3 install stanfordnlp"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stanfordnlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/bf/5d2898febb6e993fcccd90484cba3c46353658511a41430012e901824e94/stanfordnlp-0.2.0-py3-none-any.whl (158kB)\n",
            "\r\u001b[K     |██                              | 10kB 18.8MB/s eta 0:00:01\r\u001b[K     |████▏                           | 20kB 3.1MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 30kB 3.9MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 40kB 3.0MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 51kB 3.6MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 61kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 71kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 81kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 92kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 102kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 112kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 122kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 133kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 143kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 153kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 163kB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (3.10.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (1.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (4.28.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (1.17.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->stanfordnlp) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->stanfordnlp) (45.1.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (2.8)\n",
            "Installing collected packages: stanfordnlp\n",
            "Successfully installed stanfordnlp-0.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZKa5eShKHtS",
        "colab_type": "code",
        "outputId": "16290c72-60b2-45d0-cdc6-e8ff4cbbc403",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "from nltk.parse.stanford import StanfordDependencyParser\n",
        "\n",
        "path_to_jar = 'path_to/stanford-parser-full-2014-08-27/stanford-parser.jar'\n",
        "path_to_models_jar = 'path_to/stanford-parser-full-2014-08-27/stanford-parser-3.4.1-models.jar'\n",
        "\n",
        "dependency_parser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n",
        "\n",
        "result = dependency_parser.raw_parse('I shot an elephant in my sleep')\n",
        "dep = result.next()\n",
        "\n",
        "list(dep.triples())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-7d5aa47bf7d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpath_to_models_jar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'path_to/stanford-parser-full-2014-08-27/stanford-parser-3.4.1-models.jar'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdependency_parser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStanfordDependencyParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_jar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_to_jar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_models_jar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_to_models_jar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdependency_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'I shot an elephant in my sleep'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/parse/stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_to_jar, path_to_models_jar, model_path, encoding, verbose, java_options, corenlp_options)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_regex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             ),\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         )\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_jar_iter\u001b[0;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[1;32m    635\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             raise LookupError('Could not find %s jar file at %s' %\n\u001b[0;32m--> 637\u001b[0;31m                             (name_pattern, path_to_jar))\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;31m# Check environment variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: Could not find stanford-parser\\.jar jar file at path_to/stanford-parser-full-2014-08-27/stanford-parser.jar"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPE77iJcMroM",
        "colab_type": "code",
        "outputId": "83ad69ab-6408-4d76-f607-f12aee51dc39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Jim felt energetic after morning walk.\")\n",
        "for token in doc:\n",
        "    print(\"{2}({3}-{6}, {0}-{5})\".format(token.text, token.tag_, token.dep_, token.head.text, token.head.tag_, token.i+1, token.head.i+1))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nsubj(felt-2, Jim-1)\n",
            "ROOT(felt-2, felt-2)\n",
            "acomp(felt-2, energetic-3)\n",
            "mark(walk-6, after-4)\n",
            "compound(walk-6, morning-5)\n",
            "advcl(felt-2, walk-6)\n",
            "punct(felt-2, .-7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyXAQtxmNCfn",
        "colab_type": "code",
        "outputId": "81bfcb32-9f8a-4752-8f71-45785767cdfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "texts = [\"Hi, how are you #hi\", \"BERT is a new package developed by Google #hi #nlp\", \"NLP is Natural Language Processing #nlp\", \"This is my colab notebook #colab #notebook\"]\n",
        "df1 = pd.DataFrame(texts, columns = ['text'])\n",
        "df1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hi, how are you #hi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BERT is a new package developed by Google #hi ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NLP is Natural Language Processing #nlp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>This is my colab notebook #colab #notebook</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text\n",
              "0                                Hi, how are you #hi\n",
              "1  BERT is a new package developed by Google #hi ...\n",
              "2            NLP is Natural Language Processing #nlp\n",
              "3         This is my colab notebook #colab #notebook"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccZ4XWm0NDzl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1.text.str.split(expand=True).stack().value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWo50P9r2nWd",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_XnbpCqPu-E",
        "colab_type": "code",
        "outputId": "bafbb4ea-72cc-4394-f597-3d7577a08996",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "from nltk.parse.stanford import StanfordDependencyParser\n",
        "sdp = StanfordDependencyParser(path_to_jar='E:/stanford/stanford-parser-full-2015-04-20/stanford-parser.jar',\n",
        "                               path_to_models_jar='E:/stanford/stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar')    \n",
        "\n",
        "result = list(sdp.raw_parse(sentence))  \n",
        "\n",
        "# print the dependency tree\n",
        "dep_tree = [parse.tree() for parse in result][0]\n",
        "print(dep_tree)\n",
        "\n",
        "# visualize raw dependency tree\n",
        "from IPython.display import display\n",
        "display(dep_tree)\n",
        "\n",
        "# visualize annotated dependency tree (needs graphviz)\n",
        "from graphviz import Source\n",
        "dep_tree_dot_repr = [parse for parse in result][0].to_dot()\n",
        "source = Source(dep_tree_dot_repr, filename=\"dep_tree\", format=\"png\")\n",
        "source\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-ff81bbc76d56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: module 'nltk' has no attribute 'normalize_corpus'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PX2VTK30WVC8",
        "colab_type": "text"
      },
      "source": [
        "**TF - IDF for n grams**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qT1bq5Do09j_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "cf357c81-a5da-486f-f9cd-e6a826c7c959"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(norm=None, ngram_range=(3,3))\n",
        " \n",
        "new_docs = ['He watches basketball and baseball', \n",
        "            'Julie likes to play basketball', \n",
        "            'Jane loves to play baseball']\n",
        "new_term_freq_matrix = tfidf_vectorizer.fit_transform(new_docs)\n",
        "print(tfidf_vectorizer.vocabulary_)\n",
        "print(new_term_freq_matrix)\n",
        "print(new_term_freq_matrix.toarray())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'he watches basketball': 1, 'watches basketball and': 8, 'basketball and baseball': 0, 'julie likes to': 3, 'likes to play': 4, 'to play basketball': 7, 'jane loves to': 2, 'loves to play': 5, 'to play baseball': 6}\n",
            "  (0, 0)\t1.6931471805599454\n",
            "  (0, 8)\t1.6931471805599454\n",
            "  (0, 1)\t1.6931471805599454\n",
            "  (1, 7)\t1.6931471805599454\n",
            "  (1, 4)\t1.6931471805599454\n",
            "  (1, 3)\t1.6931471805599454\n",
            "  (2, 6)\t1.6931471805599454\n",
            "  (2, 5)\t1.6931471805599454\n",
            "  (2, 2)\t1.6931471805599454\n",
            "[[1.69314718 1.69314718 0.         0.         0.         0.\n",
            "  0.         0.         1.69314718]\n",
            " [0.         0.         0.         1.69314718 1.69314718 0.\n",
            "  0.         1.69314718 0.        ]\n",
            " [0.         0.         1.69314718 0.         0.         1.69314718\n",
            "  1.69314718 0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVbPjIFCWnTm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "86ca3727-ef7a-4d41-88b7-ccedc55cf887"
      },
      "source": [
        "!pip3 install sklearn"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.22.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.14.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.17.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N_QURMBWpcN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}